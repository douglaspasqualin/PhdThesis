\chapter{Related work}\label{chap:related}

This chapter presents the works related to the thesis subject. Section~\ref{sect:schedulers} describes transactional schedulers, one technique used to reduce the number of aborts during transactional execution, hence, improving the performance. Recent works use scheduling techniques for STM with the objective of optimizing resources, like cache and page sharing or to reduce the latency on data access. This kind of schedulers are described in Section~(\ref{sect:ThreadDataSTMRelated}), which presents an exhaustive list of works that uses thread and data mapping to improve the performance of STM applications. The next Section~(\ref{sect:ThreadDataRelated}) also describes works that explore thread and data mapping to improve the performance of applications that do not use STM. %However, those works are focused on general applications, not STM. 
To perform a successful mapping it is necessary to perform an in-depth analysis of STM applications, regarding sharing behavior. Hence, Section~\ref{sect:charactRelated} describes works that perform workload characterization of STM applications and sharing behavior of general applications.

\section{Transactional schedulers}\label{sect:schedulers}

Albeit a Contention Manager (CM) can help to reduce the number of aborts in a transactional execution, it has limitations. The CM acts only in a \textbf{reactive} way, dealing with a conflict when it occurs and not avoiding it \cite{Yoo:2008, Shrink, Nicacio:2012}. Transactional scheduling acts in a \textbf{proactive} way, using heuristics to prevent conflicts and to decide \emph{when} and \emph{where} a transaction should be executed \cite{Shrink}. %When the use of scheduling techniques on STM first appeared, the main idea was to avoid conflicts, and the solution, in general, was serializing a conflicting transaction. Recent works use schedulers for STM with the objective of optimizing resources, like cache and page sharing or to reduce the latency on the data access.

This section presents state-of-the-art transactional scheduling techniques. In order to identify the works on this subject, the surveys \cite{Hendler:2015} and \cite{Sanzo:2017} were used as a basis. Also, this section describe works published after the surveys. However, transactional schedulers focused on HTM, GPU (Graphics Processing Unit) or real-time systems are not included, because they are not on the scope of this thesis. Di Sanzo~\citeyearpar{Sanzo:2017} classifies the schedulers as reactive, prediction-driven, feedback-driven (all heuristic based) and machine learning or analytical (based on performance models \cite{Tay:2018}). In the next sections, we will follow this classification to describe the transactional schedulers.

\subsection{Feedback-driven}\label{sec:feedback}

Feedback techniques are based on constantly comparing if the \emph{actual behavior} of a system is the \emph{desired} \cite{Janert:2013}. Application parameters are monitored during the execution of a transaction (\emph{actual behavior}), and in each new iteration, they are used as input for the scheduler to take decisions, i.e., a corrective action in order to achieve the \emph{desired behavior}. This step is repeated during execution, trying to dynamically adjust the application \cite{Sanzo:2017}.

One of the first works to propose a transaction scheduler for  STM was the \textit{Adaptive transaction scheduling} (ATS)~\cite{Yoo:2008}, and the idea was to work together with the CM: each thread has a \emph{contention intensity} (CI), recalculated each time that a transaction finishes (commit or abort). When the CI is great than a predefined threshold, threads are inserted in a global queue to be serialized. This approach takes into consideration that when the CI is high, it is better to limit parallelism, avoiding possible conflicts. 

In \citeNamesYearPar{Ansari:2008_1}, an adaptive concurrency control (ACC) technique was proposed which limits the maximum number of concurrent threads executing transactions. The idea is that an excessive number of threads can hurt the performance in a high contention environment, mainly due to a higher number of aborts. The technique keeps track of a \emph{Transaction Commit Ratio} (TCR), i.e., the percentage of committed transactions in the total number of transactions executed, and uses it to dynamically adapt the number of concurrent threads. The ACC uses two parameters: a \emph{target TCR range} and a time \emph{interval} for calculating the TCR. Four adaptive concurrency control algorithms were proposed, varying the heuristic to change the total number of active threads, but all based on TCR. 

In \citeNamesYearPar{Ansari:2008}, the authors proposed a new concurrency control algorithm, called \emph{P-only Concurrency Controller} (\texttt{PoCC}), extending the ACC work. The main idea is to keep TCR at a configurable value (called \emph{set point}) instead of a range as in the previous work. If a high \emph{set point} is chosen, then the number of threads will quickly reduce when TCR decreases. On the other hand, it will have a slower response when TCR grows suddenly. 

\citeNamesYearPar{Chan:2011} also propose a new concurrency control technique. For this purpose, a parameter \emph{quota} is recalculated every predetermined amount of time. When a new transaction is to be started, it needs to check if there is sufficient quota available, otherwise, it will wait. There are two proposals for calculating the \emph{quota}. The first one, called \emph{Throttle}, adjusts the quota based on the commit \emph{ratio}, which is compared with a predefined threshold, and the quota is adjusted according to it. The second one, called \emph{Probe}, uses commit \emph{rate} (total of commits per unit of time) instead of \emph{ratio}. Also, it uses a try and error approach to find the best quota, instead of a fixed threshold. 

\citeNamesYearPar{Ansari:2014} published another work using TCR as the main heuristic. The difference from their first work \cite{Ansari:2008_1} and \texttt{PoCC} is that in this new algorithm, called \emph{Weighted Adaptive Concurrency Control} (\texttt{WACC}), the TCR is calculated per thread. Similar to \texttt{PoCC}, that tried to keep TCR at a configurable value (\emph{set point}), \texttt{WACC} uses the notion of \emph{expected TCR}. It predicts the global TCR that should be achieved if the determined subset of threads is activated. Thus, it is possible to predict if the \emph{set point} will be reached if the determined subset of threads will be allowed to concurrently run. Like in the first work \cite{Ansari:2008_1}, four approaches are proposed, using different heuristics (all based on TCR) to define the total number of threads. 

One of the main objectives of \citeNamesYearPar{Rito:2014}, is to avoid excessive serialization using a fine-grained approach. They have proposed the \texttt{ProPS} (\emph{Progressively Pessimistic Scheduling}) technique. In \texttt{ProPS}, a global \emph{concurrency level} ($CL$) matrix  keeps information about pairs of atomic operations $i$ and $j$. For the authors, each transaction that executes the same block of code, execute the same operation of type $i$. Accessing the matrix on the index $CL_{ij}$, it is possible to know how many transactions executing atomic operations of type $i$ could be executed concurrently with another one executing $j$ atomic operations. In the beginning, the values in the matrix are set to be equal to the maximum of threads or cores of the machine, i.e., all cores could execute all types of transactions without restriction. When a specific $CL_{ij}$ decreases (typically by an abort of $i$ or $j$), \texttt{ProPS} reduces the number of transactions executing $i$ and $j$. By design, \texttt{ProPS} reduces exponentially (\emph{progressively pessimistic}) the concurrency on aborts between atomic operations and increases it linearly at commit. 

The idea proposed by \citeNamesYearPar{Pereira:2014} is to use the \emph{percentage of effective work} (\texttt{PEW}) of a transaction as the main heuristic. The \texttt{PEW} replaces CI in approaches like \texttt{ATS}. It is calculated for each transaction, and it is based on the total of cycles executed until the transaction finishes (commit or abort). Transactions with less effective work done are prioritized. If a transaction aborts and the \texttt{PEW} is high, then it is wasting too many work (cycles) and should have a lower priority. There are queues of transactions according to their priorities. The authors noted that only using \texttt{PEW}, there is still a high number of conflicts between transactions with the same priority. Thus, an additional heuristic was included: a \emph{success-reward policy} (SRP). According to a predefined \emph{reward} threshold, a transaction could change its position in the queue. 

In \citeNamesYearPar{Ravichandran:2014}, the authors classified applications as  \emph{fully scalable} and \emph{scalability limited}. Applications under the former classification decrease their execution time as the number of cores of the machine grows. The latter is the opposite, hurting the application performance as the number of cores is increased. The authors have proposed \texttt{F2C2-STM} (Flux-based Feedback-driven Concurrency Control), which focuses on \emph{scalability limited} applications. \texttt{F2C2-STM} is inspired by TCPâ€™s (Transmission Control Protocol) network congestion control algorithm, to adjust the maximum number of concurrent threads allowed to run. The heuristic utilized is the transaction throughput. However, to compute the global throughput between all threads, a global variable is needed. To avoid synchronization, the authors chose to calculate the throughput only in one thread. Thus, they have assumed that the transactional workload is roughly the same between all threads. To find the best concurrency level, like in \citeNamesYearPar{Chan:2011}, they have used a try and error approach, monitoring the transaction throughput on each modification. 

\subsection{Reactive}\label{sec:reactive}

Reactive techniques are activated after a conflict is detected. The main objective is to avoid the same conflict to occur again~\cite{Sanzo:2017}.

The \emph{Collision Avoidance and Resolution} (\texttt{CAR-STM}) scheduling-based mechanism \cite{Dolev:2008} presents two new CMs and a scheduler. Each core has one queue, and the runtime system restricts the maximum number of threads to be equals to the cores available. The first CM proposed is called \emph{Basic}. When it detects a conflict between two transactions, it aborts the newer transaction and move it to the queue of the older. The second, called \emph{Permanent}, also aborts the newer transaction in case of conflicts. However, it marks the aborted as a subordinate of the older. If a transaction needs to be moved to another queue, \emph{Permanent} will also move all its subordinates. A proactive centralized module tries to choose the queue for a new transaction, based on a conflict probability with the running transactions on each queue. 

On Steal-on-Abort (\texttt{SoA}) \cite{Ansari:2009}, the idea is to find dynamically an optimal order to execute transactions, minimizing aborts. Each thread has two queues. The first one, called main queue, keeps new transactions that should be processed by the thread. The second queue is called ``steal''. When a transaction aborts due to a conflict, instead of restarting immediately, the opponent transaction ``steals'' the aborted one and put in the steal queue of the thread. When the opponent transaction commits, transactions on the steal queue are moved to the main queue. The authors proposed different strategies to choose in what position of the queue the stolen transaction will be put in. According to the authors, \texttt{SoA} benefits applications that have a high number of repeated conflicts. 

In Attiya and Milani~\citeyearpar{Attiya:2009,Attiya:2012}, the focus of the proposed schedulers are workloads with read-dominated transactions and lazy versioning. The authors explain that many proposed schedulers focuses on avoiding repeated conflicts, serializing transactions. Also, they do not perform well under read-dominated workloads, serializing more transactions than necessary. With this motivation, the \texttt{BIMODAL} scheduler is proposed. On \texttt{BIMODAL}, each core has a work queue. Also, a global \emph{RO-queue}, for read-only transactions is shared between all cores. When two transactions conflict, if the aborted is a writing one, it will be moved to the same queue of the conflicting. If it is read-only, it will be moved to the RO-queue. When the RO-queue reaches a threshold of enqueued transactions  (or the other queues are empty), the \texttt{BIMODAL} will prioritize the transactions in the RO-queue. The scheduler is proposed in a theoretical way. It was formally proved and compared with \texttt{SoA}.

\citeNamesYearPar{Sharp:2013} argue that prior transactional schedulers deal with concurrent conflicts only, i.e., between a reader and a writer. The objective of the authors is to deal with \emph{semantic conflicts}. This kind of conflict occurs when there is no concurrent conflict and yet it is not possible for transactions to proceed. They have cited as an example a transaction that needs to consume an item from a buffer but the buffer is empty, or a withdraw in an account without funds. With this motivation the \texttt{Hugh} scheduler is proposed. When a transaction aborts, it first needs to register itself in a \emph{transaction table}. Next, a speculative phase begins, executing a permutation of transactions that is in the \emph{transaction table}. This phase tries to find a permutation where the maximum of pending transactions can commit. Finally, in the commit phase, all successful permutations are sent to an algorithm to decide which permutation can commit. It chooses the permutation with the greatest number of transactions. The authors also proposed \texttt{Hugh2}~\cite{Sharp:2014}, using the same idea on a different STM implementation.

The \emph{second-hop conflict} is a concept proposed in the \texttt{RelSTM} scheduler \cite{Sainz:2013}. If a transaction $T_{1h}$ conflicted with $T_x$, and another $T_{2h}$ has conflicted with $T_{1h}$, then $T_{2h}$ is a \emph{second-hop} of $T_{1h}$. Upon a conflict, the transaction registers its opponent and those that have conflicted with the opponent too. When restarting, the transaction is serialized if the opponents are still running. Otherwise it could wait if a percentage of \emph{second-hop} transactions that are still running is greater than a predetermined threshold. 

\subsection{Prediction-driven}\label{sec:prediction}

Schedulers under this classification, use prediction techniques, trying to increase the probability to make the right decision on  scheduling \cite{Sanzo:2017}.

The \texttt{Shrink} scheduler \cite{Shrink} tries to predict the future memory accesses of transactions based on past accesses. The authors use the concept of temporal locality of the read-set. They have identified that multiple consecutive committed transactions in a thread access similar addresses. A per thread \textit{Bloom Filter}\footnote{\textit{Bloom Filter} is a probabilistic data structure which allows fast search and insertion of data \cite{Bloom:1970}.} keeps track of the last read addresses. When an address is read, \texttt{Shrink} verifies if it is in the \textit{Bloom Filter}. If true, it was recently accessed and it is included in the \emph{predicted read-set} of the thread. In case of abort, the write-set is copied to the \emph{predicted write-set}. Each thread has a \emph{commit rate}, calculated every time that a transaction commits or aborts. When the commit rate is greater than a predefined threshold, \texttt{Shrink} verifies, before starting a transaction, if some address in the prediction sets (read or write) are being written by other threads. If true, the serialization is activated. 

In \citeauthoronline{Heber:2012}~\citeyearpar{Heber:2009,Heber:2012}, the focus is to avoid too much oscillation between serialization and non-serialization periods. The authors proposed a mechanism called \emph{Low-Overhead Serializing} algorithm (LO-SER) which aims to keep certain stability between serialization times. The scheduler collects statistics like past commits and aborts. This data can be local (per thread) or global. The proposed stabilizing mechanism calculates a \emph{contention level} (CL) each time that a transaction aborts or commits. Then, a low ($lt$) and a high threshold ($ht$) needs to be defined, i.e., a range. Serialization is applied if CL is greater than $ht$ and deactivated when CL is lower than $lt$. 

\citeNamesYearPar{Atoofian:2011} uses the intuition that if a transaction aborted, it will fail again in the future. This theory is called \emph{locality of contention}. To avoid it, it is important that the transaction that caused the abort should finish before restarting the aborted. With this motivation, the \emph{Speculative Contention Avoidance} (\texttt{SCA}) mechanism is proposed. Each thread has a Contention Predictor (CP), composed by a \emph{contention bit} (CB) and a \emph{saturating counter} (SC). The CB shows the result of the last transaction executed on the thread (1 if failed or 0 if committed). The SC is similar to branch predictors used in pipelines processors \cite{Yeh:1992}. It is incremented each time that a transaction conflicts and zeroed when commits. Before a thread executes, it consults the \texttt{SCA}, that will serialize the transaction if CB is equals to 1 and SC is greater than a given threshold. 


\subsection{Mixed Heuristics}\label{sec:mixedHeuristic}
This section describes works that use mixed strategies according to different transaction profiles. For instance, a transaction initially uses a lightweight feedback-driven technique. However, if the size of a transaction (for instance, based on the amount of memory locations read) is above a given threshold, the system switches to a more complex heuristic.

In \citeauthoronline{Nicacio:2011}~\citeyearpar{Nicacio:2011,Nicacio:2012}, the \emph{Light-Weight User-Level Transaction Scheduler} (\texttt{LUTS}) is proposed. In \texttt{LUTS}, each transaction that executes the same critical section shares an identifier (ID). The main idea is to have different scheduling heuristics according to  transaction length. The number of cycles  is utilized to define if a transaction is long or short. If it is considered short, the heuristic used is similar to \texttt{ATS}, using a contention intensity (CI). The difference is that the CI is calculated per transaction ID, instead of per thread like in the original \texttt{ATS}. Moreover, if a transaction is serialized, \texttt{LUTS} tries to choose one with different ID to replace it. If a transaction is considered long, a more sophisticated (and expensive) heuristic that uses additional data structures is utilized, trying to predict and avoid conflicts. The system uses a fixed thread that is responsible for calculating the length of transactions. Another feature, like in \texttt{CAR-STM}, is to restrict the maximum number of threads to be equal to the cores available.

An extension for \texttt{LUTS} was proposed in \cite{Pereira:2013}, called \texttt{BAT} (\emph{Best Alternative Transaction}). In \texttt{LUTS}, the CI is utilized for short transactions only. \texttt{BAT} utilize the CI for long transactions too, as an additional parameter for helping to choose the best transaction to be scheduled. In other words, CI is calculated for all transactions. For short ones, it is the unique parameter used to decide which transaction is allowed to run, i.e., with less probability of conflicts. For long transactions, there are more data structures used together with CI to decide it. 

The \texttt{ProVIT} scheduler \cite{Rito:2015} uses the same idea from \texttt{LUTS}, where there are distinct heuristics according to transaction lengths. However, in \texttt{ProVIT} it is possible to have two heuristics active at the same time, i.e., each thread using a different heuristic. A transaction is considered long if the size of their read-set is greater than a predefined threshold. This information is updated dynamically and, on start, transactions are considered short. For short transactions, the heuristic is based on the previous work of the authors, \texttt{ProPS}, described in Section \ref{sec:feedback}. When a transaction aborts, the read-set is verified to check if it is a long transaction. If true, it is marked as a VIT (\emph{Very Important Transaction}) and its read-set is copied to a data structure available for all threads. Before a writing transaction commits, it needs to check if its write-set has any intersection with the read-set of any VIT transaction. If it has, the commit should be delayed, giving priority to the VIT. The idea is to avoid a VIT to abort again. 

\subsection{Others}\label{sec:otherSchedule}

This Section describes works that use machine learning (ML), analytical performance models of  applications or other techniques. As these techniques are out of scope of this thesis, they will not be described in details.

Extending the Linux Kernel for supporting STM scheduling was explored in \citeauthoronline{Maldonado:2011}~\citeyearpar{Maldonado:2010,Maldonado:2011}.

The idea of using ML and analytical models in schedulers was widely explored by Rughetti and Sanzo. Controlling the total of threads allowed to concurrently run was proposed in \texttt{SAC-STM} \cite{Rughetti:2012}, using neural networks, and in \texttt{CSR-STM} \cite{Sanzo:2013} with analytical performance models. Also, in \citeNamesYearPar{Rughetti:2014} a work integrating \texttt{SAC-STM} and \texttt{CSR-STM}, named \texttt{AML} was proposed. Later, \citeNamesYearPar{Rughetti:2014_2} describe an extension to \texttt{SAC-STM}, called \texttt{DSF-STM}. In Di Sanzo et al.~\citeyearpar{Sanzo:2016,Sanzo:2020}, a Markov chain-based analytical performance model is proposed to dynamically control the total of concurrent threads executing transactions allowed to run. \citeauthoronline{Castro:2011}~\citeyearpar{Castro:2011,Castro:2012} have used ML techniques to choose  the best \emph{thread mapping} for improving  STM performance. \citeNamesYearPar{Popovic:2017} and \citeNamesYearPar{Popovic:2019}, proposed schedulers for Python-STM~\cite{PythonSTM}, using ML techniques.

Finally, \citeNamesYearPar{Marques:2016}, have proposed \emph{Dynamic Serializer} (DS). Different from prior approaches, they have focused on reducing energy consumption instead of performance.

\section{Thread and data mapping in STM}\label{sect:ThreadDataSTMRelated}

The works studied so far use transactional scheduling to avoid conflicts between transactions. This section describes works which aim to use scheduler techniques on STM focusing on optimizing resources. For example, trying to keep threads on sibling cores to share caches or load balance in multiprocessor systems.

\citeNamesYearPar{Castro:2014} have studied how \emph{thread mapping} could be utilized for improving STM performance. First of all, they describe four different possible thread mappings that could be utilized: Scatter, Compact, Round-Robin and Linux. The proposed mechanism dynamically collects information to decide the mappings. The first strategy was called \emph{Conflict}. It uses the \emph{abort ratio} (AR) as the main heuristic. The intuition utilized is: if AR is high (based on a threshold), the application is accessing a great quantity of shared data. In these cases, using a \emph{Compact} mapping could be useful for sharing caches. If AR is moderated, \emph{Round-Robin}, the intermediate solution, is used; otherwise \emph{Scatter}. The second strategy is called \emph{Test}. It uses the three mappings on a fixed period of time and computes the execution time of each of them. At the end, the mapping that had the minor execution time is selected. 

In \citeNamesYearPar{Chan:2015} a thread mapping mechanism called \emph{Affinity-Aware Thread Migration} is proposed. It aims to detect threads that access common data, keeping them in sibling cores on the same processor to share caches. To compute which threads are sharing data, they have used a matrix $n \times n$, where $n$ is the maximum of threads that the system supports. When a thread $i$ conflicts with a $j$, i.e., aborts, the respective index $i,j$ is updated in the matrix, using the CI (contention intensity) proposed in \texttt{ATS} (Section \ref{sec:feedback}). The intuition used is: if two threads conflict, they were accessing the same shared data. This matrix represents a graph, with edges and their weights. The objective is to reduce the sum of edges between processors. Only a pair is migrated per time. 

In \citeNamesYearPar{Zhou:2018} a concurrency control mechanism to dynamically adjust the total number of active threads executing transactions is proposed. Also, as the total of threads changes, the authors also adjust thread mapping. The mappings utilized are the same from \citeNamesYearPar{Castro:2014}, i.e, \emph{Scatter}, \emph{Compact}, \emph{Round-Robin} and \emph{Linux}. The heuristic for deciding the total number of active threads is based on \emph{commit ratio}~(CR) and \emph{throughput}. In the beginning, two or more threads (configurable) are allowed to run concurrently. Each period of time, the CR and the throughout are computed, adjusting the total of threads according to it. For thread mapping, the STM system is initialized with the \emph{Linux} default mapping, and after a certain time, it changes to \emph{Round-Robin} (intermediate solution). If the throughput is higher, then \emph{Compact} is used. If performance decreases, then \emph{Scatter} is used. 

\citeNamesYearPar{Goes:2014} focused on STM applications that have a worklist pattern, i.e., they have a single operation responsible to process an item of work from a dynamically managed worklist. Regarding \texttt{STAMP} benchmark, four applications present this pattern: \emph{intruder}, \emph{kmeans}, \emph{vacation}, and \emph{yada}. The idea is encapsulated STM application as a skeleton framework, improving the memory affinity by using static page allocation (bind or cyclic) and data prefetching by using helper threads. Helper threads make use of idle cores to bring potentially useful data for the LLC. The memory affinity mechanism is called \texttt{SkelAff} and it was implemented inside a proposed \texttt{OpenSkel} framework. Hence, STM applications need to be rewritten with this framework to be able to use the memory affinity improvements. It is worth noting this proposed framework is only able to deal with one specific kind of sharing pattern.

\section{Thread and data mapping in general applications}\label{sect:ThreadDataRelated}

This section presents works that use thread and/or data mapping to improve data locality of applications that do not use STM, running on shared memory architectures. To identify the works on this subject, the surveys \cite{Diener:2016Sur} and \cite{Cruz:2018} were used as a basis. We exclude two works described in the papers, because they were out of scope: compiler analysis and fixed runtime options. The former relies on complex analysis that will modify the entire application and not only the STM implementation. Also, this technique usually is limited to specific compiler versions \cite{Diener:2016Sur}. The latter is used to specify global and static policies, that will be kept fixed until the application finishes, such as interleave for data or round-robin for thread mapping. The remaining  techniques will be grouped into two groups: offline and online techniques. Finally, works that focuses on distributed memory, like \emph{Message Passing Interface} (MPI) will not be included, as this thesis does not focus on STM for distributed systems. Although MPI can be used for shared memory architectures, the form of communication is explicit, i.e, tracking the sent messages is possible to discover threads the are communicating often~\cite{Cruz:2018}.

\subsection{Offline Techniques}\label{sect:offlineTec}

This kind of technique consists of analyzing the source code of the application or profiling and executing it, to identify the memory access behavior \cite{Diener:2016Sur}. 
Then, the application is modified, either manually or automatically, applying an improved mapping.  Behavior analysis happens before execution. At run time, no profile information needs to be collected. However, if the application changes its behavior dynamically, the profile phase will fail on getting precise information about memory access \cite{Diener:2016Sur}. Profiling could be implemented manually, using system call functions or with the help of instrumentation tools like \texttt{Pin} \cite{Luk:2005}.

Regarding manually analyzing and source code modifications, some tools could help programmers in this task. \texttt{libnuma} \cite{libnuma} is a library and command-line tool that is a wrapper for kernel system calls to set memory policies on NUMA architectures. Using \texttt{libnuma} as a library, it is possible to allocate memory on a specific NUMA node and the policy to be used, like interleave. Also, it is possible to bind a thread to run on the CPUs of a specific NUMA node. Ribeiro et al. \citeyearpar{Ribeiro:2009,Ribeiro:2011} have proposed the tools \texttt{MAi} (Memory Affinity interface) and \texttt{Minas}. \texttt{MAi} is a user-level interface focused on numerical scientific HPC applications. It provides optimizations for arrays and memory policies to manage data allocation. The main optimization was made on arrays. When using \texttt{MAi} to allocate memory for an array, the programmer can specify the data distribution, i.e., how rows and columns should be distributed in the NUMA nodes. Also, \texttt{MAi} implements thread scheduling to ensure data locality. \texttt{Minas} is a portable framework for managing memory affinities explicitly or automatically on NUMA architectures. \texttt{Minas} has a preprocessor to perform automatic source code modifications and optimizations analyzing the NUMA architecture where the application is compiled. For manual controlling of the memory affinity in the source code, \texttt{Minas} uses \texttt{MAi}. Another library is \texttt{TBB-NUMA} \cite{Majo:2015, Majo:2017}, that is an extension of the Intel Threading Building Blocks (TBB) \cite{Reinders:2007}. It includes functionalities for manual management of data allocation between NUMA nodes, automatic thread mapping and other features to make the library NUMA-aware. 

Two works use the described tools to achieve better thread and data affinity. \citeNamesYearPar{Dupros:2010} have studied the impact of memory affinity in seismic simulations. After a detailed study of the source code and the underlying NUMA architecture where the application was executed, \texttt{MAi} was utilized for efficient thread and data mapping of the application. \citeNamesYearPar{Cruz:2011} made similar studies on the NAS Parallel Benchmark (NPB) \cite{NPB:1999}, and after the detailed analysis of the applications, they have used \texttt{Minas} for applying the improved thread and data mapping.

There are works that first execute an application to get detailed information about the memory access pattern. Then, this information is used to improve performance. \citeNamesYearPar{Diener:2010} recorded in a communication matrix which threads have accessed the same memory location. After that, an algorithm uses this matrix to find the best placement of the threads. If a machine support 16 threads at maximum, then it is feasible to do an exhaustive search, i.e., trying every possible placement. Otherwise, an heuristic is applied. \citeNamesYearPar{Majo:2012} have studied the access pattern of scientific benchmarks. The applications were profiled using hardware counters of the Intel Nehalem processor. Also, the source code of the applications were studied in detail, to identify the access patterns of matrices, a common data structure on scientific computation. With this information, the authors have proposed new primitives to be used in OpenMP, to have better control on data distribution and  loop iterations over the data structures. \citeNamesYearPar{Mariano:2016} have made a detailed study in the source code of the HashSieve algorithm, which is used in the context of the lattice-based cryptography technique~\cite{Micciancio:2009}. The optimization was made by reordering some operations to leverage cache locality, prefetching data and changing data structures used in the algorithm. \citeNamesYearPar{Denoyelle:2019} have created ML models to choose the best placement of threads and data of applications. They have used different types of thread placement (compact and scatter), data policies (first-touch and interleave) totaling 4 combinations. The application is executed using a default configuration (compact and first-touch) and a set of metrics are collected using instrumentation and hardware counters. Hence, these metrics are processed and used as an input parameter to mathematical and machine learning models. These trained models can identify the best placement for the applications.

Analyzing tools were proposed to collect information about  memory access of  applications. These tools help to understand how applications share data. \texttt{Numalize} \cite{Diener2015} is based on the Pin instrumentation tool \cite{Luk:2005}. The authors also proposed metrics to characterize communication and page usage of applications. Thus, \texttt{Numalize} generates a memory trace of applications and calculates the proposed metrics. The generated information helps to choose the best placement of threads and data. \texttt{TABARNAC}~\cite{Beniamine:2015} is similar to \texttt{Numalize} and provides graphical visualization of memory access behavior, like the distribution of the accesses by threads and data structures. A most recent tool, \texttt{NumaMMa} (NUMA MeMory Analyzer) \cite{Trahay:2018} uses hardware counters of a processor to generate memory traces. When the application finishes, \texttt{NumaMMa} processes the trace and analyzes the cost of memory accesses of each object and how threads access them. Also, graphical visualization of the processed information is available.

\citeNamesYearPar{Denoyelle:2019}, obtain information about the characteristics of applications through a preliminary execution. This information is collected using instrumentation or hardware performance counters. After that, using machine learning, the collected information is processed and, resulting in information if application is sensitive to locality and the best thread and data placement. Then, the application is re-executed using the proposed placement.


\subsection{Online Techniques}\label{sect:onlineTec}

Online techniques collect information about  memory access during the execution of the application and perform  mappings dynamically. The main advantage is that no prior execution is needed to collect information. However, the main challenge is to create an heuristic that takes into consideration the trade-off between accuracy and runtime overhead \cite{Diener:2016Sur}. Analyzing all memory accesses of an application is the best strategy for deciding mappings, but the overhead added makes it unfeasible \cite{Cruz:2018}. Thus, usually, the methods used to collect information about memory accesses is based on sampling. Also, the migration of threads and  pages represent an overhead that should be considered.

\citeNamesYearPar{Ogasawara:2009} have used the Garbage Collector (GC) of the Java programming language to identify the preferred NUMA node for objects. During the collection phase, the proposed method determines which threads have most accessed certain objects, i.e., the \emph{Dominant Thread} (DoT). Then, the GC migrates the objects to the NUMA nodes where the DoT is running.

\texttt{ForestGOMP} \cite{Broquedis:2010} is an extension of OpenMP that uses hints provided by application programmers, compiler techniques and hardware counters to perform dynamically thread and memory placement. It uses internally libraries like \texttt{hwloc} to compute the underlying hardware hierarchy and the \texttt{BubbleScheduler} framework \cite{Thibault:2007}. Threads that share data or synchronize often are organized in bubbles. The main objective of the proposed scheduler used by \texttt{ForestGOMP} is to improve the cache usage by making each ``bubble'' access the local memory, migrating pages if necessary. The scheduling actions are triggered when resources are (de)allocated, a processor becomes idle or a hardware counter exceeds a predefined threshold.

\texttt{autopin} \cite{Klug:2011} is a framework that extends the \texttt{pfmon} utility from the \texttt{perfmon2} package \cite{Eranian:2005}. \texttt{pfmon} is a command line tool that accesses hardware performance counters of processors. Through an environment variable it is possible to choose a set of mappings, i.e., the cores to be used and the order that they should be assigned to threads. A command-line parameter is available for setting a hardware counter that \texttt{autopin} should use for calculating the performance rate. After the initialization, if more than one set of mappings was defined, \texttt{autopin} will test each one, calculating the performance based on the timestamp and the hardware counter chosen. After all mappings have been tested, the one which achieves the best performance will be chosen and will be active until the application finishes. 

The focus of the \texttt{BlackBox} scheduler \cite{Radojkovic:2013} is applications with a low number of threads and many instances, like networking applications. It tries to test all possible thread mappings (assignments) and chooses the one that achieves the best performance. It has 3 distinct phases. The first one profiles the target application. The main information collected are conflicts and shared resources, like caches and memory controllers. The second phase uses the information collected to predict the performance of a determined thread assignment. If the number of threads and instances are low, all possibilities are tested. Otherwise, only 1000 combinations are evaluated. The last phase chooses the best 5 predicted models and tests them to make sure that the best one was chosen.

\citeNamesYearPar{Tam:2007} have used hardware performance counters of the IBM Power5 processor to track cache misses between chips, i.e., if the miss was local or remote. Each thread has a data structure to keep track of data regions accessed that caused a remote cache miss. This information is retrieved from a counter in the processor. Analyzing all cache misses would be unfeasible, thus, the authors sampled the monitoring. The next step is to analyze data structures looking for data sharing between threads. If found, threads are mapped to the same chip, for sharing caches. After that, the monitoring phase starts again. The proposed scheduler was tested with a synthetic micro-benchmark and three commercial server workloads. In \citeNamesYearPar{Azimi:2009} the authors have extended the studies of how the hardware performance counters could be utilized for improving locality of threads. They have presented the usability and importance of hardware performance counters as a low-overhead resource to get accurate online information about performance.

Regarding data mapping, \citeNamesYearPar{Lof:2005} have studied how a next-touch directive could improve the performance of industrial applications. 
One application was chosen that uses a large matrix initialized by the main thread. After the initialization, it consumes about 500 MB of memory. Using the default Linux OS data mapping, first-touch, all pages will be allocated in the first node. With next-touch, the data will be migrated according to the accesses of threads. The author has identified a great overhead on page migrations, due to TLB (translation look-aside buffer) shootdowns, i.e., the mechanism that keeps the TLB's coherent \cite{Villavieja:2011}. To overcome this limitation, they have proposed to use larger pages, e.g., 64Kb instead of 8Kb.

The \emph{Communication Detection in Shared Memory} (CDSM)~\cite{cdsm} is a Linux kernel module to detect communication patterns of parallel applications and migrate threads according to their memory affinity. It keeps track of page faults to detect the communication between threads. The \emph{kernel Memory Affinity Framework} (kMAF)~\cite{kmaf} is implemented directly in the Linux kernel, extending the idea of CDSM to the problem of data mapping in NUMA architectures. The \emph{Carrefour} mechanism~\cite{Dashti:2013} is also implemented directly in the Linux kernel. However, it uses Instruction-Based Sampling (IBS) available only in AMD processors. \citeNamesYearPar{Barrera:2020} have the same objective of \citeNamesYearPar{Denoyelle:2019} (described in Section~\ref{sect:offlineTec}), however their model works online, and they have added prefetching optimizations beyond thread and data placement.


%\texttt{ForestGOMP}~\cite{Broquedis:2010} is an extension of the OpenMP runtime that uses hints provided by programmers, compiler techniques, and hardware counters to perform dynamically thread and memory placement. Threads that share data or synchronize often are organized in bubbles. The main objective of the proposed scheduler used by \texttt{ForestGOMP} is to improve the cache usage and try to make each ``bubble'' access local memory, migrating pages if necessary. The focus of the \texttt{BlackBox} scheduler~\cite{Radojkovic:2013} is applications with a low number of threads and many instances, like networking applications. It tries to test all possible thread mappings and chooses the one that achieves the best performance. If the number of threads and instances are low, all possibilities are tested. Otherwise, only 1000 combinations are evaluated.

%The \emph{Communication Detection in Shared Memory} (CDSM)~\cite{cdsm} is a Linux kernel module to detect communication patterns of parallel applications and migrate threads according to their memory affinity. It keeps track of page faults to detect the communication between threads. The \emph{kernel Memory Affinity Framework} (kMAF)~\cite{kmaf} is implemented directly in the Linux kernel, extending the idea of CDSM to the problem of data mapping in NUMA architectures. The \emph{Carrefour} mechanism~\cite{Dashti:2013} is also implemented directly in the Linux kernel. However, it uses Instruction-Based Sampling (IBS) available only in AMD processors.

%Some works first execute an application to get detailed information about the memory access pattern (profiling). Then, the application is re-executed, using the collected information to improve performance. Diener et al. \cite{Diener:2010} recorded in a communication matrix which threads have accessed the same memory location. After that, an algorithm uses this matrix to find the best placement of the threads to cores.
%\citeNamesYearPar{Majo:2012} have studied the access pattern of scientific benchmarks. The applications were profiled using hardware counters. Also, the source code of the applications were studied in detail, to identify the access patterns of matrices, a common data structures on scientific computation.
%With the collected information the authors have proposed new primitives to be used in OpenMP, to have better control on data distribution and loop iterations over data structures. \citeNamesYearPar{Mariano:2016} have made a detailed study in the source code of the HashSieve algorithm, which is used in the context of the lattice-based cryptography technique~\cite{Micciancio:2009}.
%The optimization was made by reordering some operations to leverage cache locality, prefetching data and changing data structures used in the algorithm. \citeNamesYearPar{Denoyelle:2019}, obtain information about the characteristics of the applications through a preliminary execution. This information is collected using instrumentation or hardware performance counters. After that, using machine learning, the collected information is processed and, resulting in information if application is sensitive to locality and the best thread and data placement. Then, the application is re-executed using the proposed placement. \citeNamesYearPar{Barrera:2020} have similar goals, however their model works online, i.e., during runtime and they added prefetching optimizations beyond thread and data placement.

\section{Workload characterization}\label{sect:charactRelated}

In \citeNamesYearPar{Williams:2009}, the Splash-2 and Parsec benchmark suites were characterized regarding their memory access behavior. They also showed other characteristics such as temporal and spatial characterization of the communication. All information was gathered through the use of a simulator. Using the collected information, they proposed changes to the communication systems to be used in future chip multiprocessors (CMP). Different communication characteristics of Splash-2 and Parsec were also studied in \cite{Mohammed:2015}. \citeNamesYearPar{Rane:2011} proposed to instrument the source code of applications using the LLVM compiler, to extract memory trace of applications. Then, they compute metrics using the collected data, such as cycles per access; NUMA hit ratios; access strides, etc. With this information, they manually changed a subset of programs of the Rodinia benchmark suite and were able to improve the performance. The  \texttt{numalize} tool to extract information about communication behavior was proposed in \cite{Diener2015}. Also, they have analyzed different benchmark suites and proposed metrics that describes spatial, temporal, and volume properties of memory accesses to shared areas.

\citeNamesYearPar{Hughes:2009} proposed a set of transactional characteristics to classify the similarity of transactional workloads. The main idea is to select a subset of programs that present distinct transactional characteristics, to be used in benchmarks or tests. In \cite{Castro:2011}, a generic mechanism was proposed to intercept any function of STM libraries. The mechanism is implemented as an external tool, allowing developers to extract and calculate information accessed inside the STM library. \citeNamesYearPar{Baldassin:2015} characterized the memory allocation of \texttt{STAMP}.

Other studies aim to characterize the energy consumption of STM workloads. \citeNamesYearPar{Baldassin:2012} studied the difference of energy consumption of the \texttt{STAMP} benchmark using three STM libraries, along with three different conflict resolution scheme.  This analysis was made using a simulator. As a result of the studies, they have proposed to integrate a \emph{dynamic voltage and frequency scaling} (DVFS) inside the contention manager, to improve the energy efficiency and performance. \citeNamesYearPar{Rico:2015} also studied the energy consumption of STM using the \texttt{STAMP} benchmark. However, the study was made using a real machine, instead of a simulator.

\section{Discussion}

As described in Section~\ref{sect:schedulers} a transactional scheduler is a well-known technique for improving the performance of STM. The majority of described transactional schedulers focuses on serializing transactions to avoid conflicts. They rely on the idea that aborting a transaction is a waste of time and resources, and it is necessary to avoid it. Although avoiding aborts in an STM execution is also important for improving performance, in current multicore architectures with complex memory hierarchies and different latencies, it is also important to consider where the memory of a program is allocated and how it is accessed. Hence, our idea is to work with thread mapping in STM, investigating the use of sharing-aware mapping~(Section~\ref{sect:sharing-aware}) in an STM implementation. This technique aims to map threads to cores and memory pages to NUMA nodes considering their memory access behavior.

\begin{table}[!tb]
	\small
	\centering
	\caption{Comparison of related work on thread and data mapping for STM applications.}
	\label{tab:comparisonRelated}
	\begin{tabular}{@{}lrccc@{}}
		\toprule
		\textbf{Work} & \multicolumn{1}{l}{\textbf{Year}} & \multicolumn{1}{l}{\textbf{Thread mapping}} & \multicolumn{1}{l}{\textbf{Data Mapping}} & \multicolumn{1}{l}	{\textbf{Sharing based}} \\ \midrule
		\citeauthoronline{Castro:2014} & \citeyear{Castro:2014}   & X                                          &                                           &                                            \\
		\citeauthoronline{Goes:2014}   & \citeyear{Goes:2014}     & X                                           & P                                         &                                            \\
		\citeauthoronline{Chan:2015}   & \citeyear{Chan:2015}  & X                                           &                                           & P                                          \\
		\citeauthoronline{Zhou:2018}   & \citeyear{Zhou:2018}   & X                                           &                                           	&                                            \\
		\textbf{This thesis} & 2021                              & X                                           & P                                         & 	X                                          \\ \bottomrule
	\end{tabular}
\end{table}

\tablename~\ref{tab:comparisonRelated} makes a direct comparison of our proposed work with the related work on thread and data mapping for STM (Section~\ref{sect:ThreadDataSTMRelated}). The works of \citeNamesYearPar{Castro:2014} and \citeNamesYearPar{Zhou:2018} do not take into consideration memory access behavior to decide the thread mapping (column \textit{Sharing based}). \citeNamesYearPar{Goes:2014} focused on a specific sharing pattern (\textit{worklist}) and their mechanism to exploit memory affinity were implemented in a framework. Hence, applications need to be rewritten with this framework to be able to use the memory affinity improvements. Also, their data mapping is based on static page allocation. Thus, we classified data mapping support in their work as partial (\textit{P}) in \tablename~\ref{tab:comparisonRelated}. %Our idea is to deal with any kind of sharing pattern and implement the modifications inside the STM runtime
We propose not to rely on a specific sharing pattern, as we intend to calculate the thread mapping based on the sharing pattern, stored in a communication matrix~(Section~\ref{sect:commMatrix}). Also, it would not be necessary to rewrite the STM application, only recompile it, since the mechanism for thread mapping will be integrated into the respective STM runtime. The main focus of this thesis is on thread mapping. However, we show how data mapping can be added to the proposed mechanism of sharing-aware thread mapping. Hence, we classified our data mapping support as partial (\textit{P}).  Finally, \citeNamesYearPar{Chan:2015} take into consideration the memory access behavior of applications. 
%However, their mechanism is only triggered by aborts. Thus, we classified their work partial (\textit{P}) \textit{sharing based} in \tablename~\ref{tab:comparisonRelated}. Our idea is to trigger the mechanism on each transactional read or write, making our proposal more accurate. Furthermore, our proposed mapping is global, i.e., taking into consideration all threads, not only one pair each time.  
However, they proposed to compute the sharing behavior only when a transaction aborts. The intuition is that if two transactions conflict, they were accessing the same shared variable. The disadvantage is that their mechanism does not capture the sharing behavior between read-only transactions. Besides, in low contention workloads, applications present a low number of aborts. Therefore, their mechanism could not capture an accurate sharing behavior of these STM applications. Our idea is to compute the sharing behavior using transactional reads and writes, making it more accurate. Furthermore, our proposed mapping is global, i.e., taking into consideration all threads, not only one pair each time.

Concerning the characterization of STM applications regarding sharing behavior, Section~\ref{sect:charactRelated} showed that so far, no works in the literature were proposed with this objective.


\section{Summary}
This chapter presented the related work on the thesis subject. There are several works on transactional schedulers. Although reducing the number of aborts improves the performance, recent works use schedulers (thread and data mapping) with the objective of improve cache usage, page sharing or reduce the latency on data access. However, no work on STM applications takes into consideration memory access behavior to performing thread and data mapping. Besides, this chapter showed that no related work performed a characterization of STM applications regarding sharing behavior. The next chapters will explore these research opportunities to advance the state-of-the-start on this subject.
